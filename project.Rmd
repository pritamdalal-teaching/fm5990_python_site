---
title: "Project"
output: html_document
editor_options: 
  chunk_output_type: console
---

<br>

<!-- ### Project Parameters -->


**Assigned:** Tuesday 3/5

**Due Date:** Tuesday 4/2 by 5:30p.

**Format:** e-mail me a one zip files per group containing:
    
1. One Jupyter notebook for each of the underlyings you analyzed.  Each notebook will contain the analysis for one underlying, including prose that explains code and visualizations.  **Every** code cell or visualization should have text cells before and/or after the provide explanation.  Any graph should be labeled. 

The notebook should be self-contained and should run easily. If your notebook doesn't run easily, I will not feel obliged to provide feedback.

1. A folder called `project_data` that contains all data that goes into your analysis.

**Guidance:** I'm intentionally not giving many speifics instructions for the the project.  This is to mimic what you face in real-world data analysis: confusing data sets that you have to try to make sense of.  My intention is that you will understand the data from your own exploration and by asking me questions.

Over the next several weeks, many of the in-class tutorials will be relevant to this project.  I will also provide opportunities to ask questions about the project during each class meeting. 


<br>

#### Groups

- Groups can be of any size (including solo).  Have someone from your group e-mail me your group members.

- Each individual will be assigned one underlying to analyze.  You can collectively analyze your underlyings, but turn in one separate notebook per underlying.  

- I will assign each individual an underlying once you e-mail me your groups.

- If you don't have a group and would like one, please e-mail me.

<br>

#### Self-Directed Projects
For those of you choosing to do your own project, that's great - I truly believe that the best way to learn data analysis is to have a project that you personally care about.  However, obviously there is more required of you to determine the steps involved.  I am happy to provide guidance in any way that I can.

For working professionals in the class, if you would like to do something that is directly relevant to your work, that's great too.  If it involves proprietary data, or can only be done on your work PC, that's fine.  Just type up a description in a Jupyter notebook and e-mail it to me.

If you are going to choose your own project, please send me an e-mail letting me know what it is. 

<br>

#### Data Set
This data set consists of the PNLs for selling weekly 30-delta strangles on various underlyings over a period of 5 years.  Each individual is responsible for answering the questions below for one underlying (which I will assign to you).

[Project Data](project_data.zip)

<br>

#### Questions to Answer
1. All the underlyings are ETFs.  Write a 1-2 sentence summary of the investment strategy of your underlying.

1. Check for large discrepancies between implied vols and the volatility index. How many of these large discrepancies occured during periods of high market turbulence?

1. Identify the five worst months in terms of PNL (group by month/year of expiration date).  Determine if losses were for idosyncratic reasons or systemic reasons.

1. Bonus: Demonstrate the presence of the leverage effect in your underlying by regressing returns against changes in the volatility index.


<!-- **Dates/Expirations:** The includes a total of 260 expirations: 1/10/2014-12/28/2018. -->

<!-- **Underlyings:** There is data for 50 ETF underlyings included in the data set.  You must include at least 20 underlyings in your analysis.  The list of underlyings can be found in `project_underlying.csv`. -->

<!-- **Trades:** The trades have already been calculated for  `project_trade_master.csv`. (This was the first part of your initial assignment, albeit for a different time period.) -->

<!-- **PNLs:** The PNLs for the trades have already been calculated for you and can be found in `project_strangle_pnl.csv`. (This was the second part of your initial assignment, albeit for a different time period.)  There are a variety of pnl related columns in this dataset; the only ones that you need to worry about are `dly_opt_pnl`, `ttd_opt_pnl`, and `strangle_unity_mult`. -->

<!-- **Position Size Scaling:** The prices of our underlyings vary considerably, and therefore so do the prices of their options.  For the backtest, I wanted each strangle for each underlying to be approximately the same size.  In particular, I created a position scaling method that would make each strangle sale to result in about $0.90 in premium. Position scaling data is stored in `project_position_scaling.csv`.  -->

<!-- Here is some code to see the min, max, and average premium, sold for each variation, for all 50 underlyings. -->
